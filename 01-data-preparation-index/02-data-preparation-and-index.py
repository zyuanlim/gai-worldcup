# Databricks notebook source
# MAGIC %pip install transformers==4.40.1 langchain==0.1.17 databricks-vectorsearch==0.33 mlflow==2.12.1
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingesting HTML files as text format

# COMMAND ----------

# DBTITLE 1,Ingesting HTML files as text format
volume_folder = '/Volumes/llm_hackathon/default'

df = (spark.readStream
        .format('cloudFiles')
        .option('cloudFiles.format', 'BINARYFILE')
        .option("pathGlobFilter", "*.md")
        .load(f"dbfs:{volume_folder}/edgar/mds"))

# Write the data as a Delta table
(df.writeStream
  .trigger(availableNow=True)
  .option("checkpointLocation", f'dbfs:{volume_folder}/checkpoints/edgar_raw_docs')
  .table('edgar_md').awaitTermination())

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM edgar_md LIMIT 5

# COMMAND ----------

import io
from pyspark.sql.functions import split, replace, element_at, lit, array_join, explode
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, OpenAIGPTTokenizer
import pandas as pd
from pyspark.sql.functions import pandas_udf

# COMMAND ----------

df_md = spark.table("edgar_md")
df_metadata = spark.table("edgar_metadata")

# COMMAND ----------

df_md.select('path', 'length').show(10, truncate=False)

# COMMAND ----------

df_metadata.show(10)

# COMMAND ----------

df_md = df_md.withColumn('accession_no', replace(element_at(split(df_md.path, '/'), -1), lit('.md'), lit('')))

# COMMAND ----------

df = df_md.join(df_metadata, 'accession_no', 'inner')

# COMMAND ----------

df.describe().show()

# COMMAND ----------

df = (df.withColumn('content_string', df.content.cast('string'))
        .withColumn('cik', df.cik.cast('string'))
        .withColumn('tickers', array_join(df.tickers, ', '))
        .withColumn('exchanges', element_at(df.exchanges, 1)))

# COMMAND ----------

max_chunk_size = 500
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
    ("####", "Header 4"),
]

tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt")
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)
md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)

# Split on headers, but merge small chunks together to avoid too small chunks.
def split_md(md, min_chunk_size=20, max_chunk_size=500):
    if not md:
        return []
    header_chunks = md_splitter.split_text(md)
    chunks = []
    previous_chunk = ""
    # Merge chunks together to add text before header and avoid too small docs.
    for c in header_chunks:
        content = c.metadata.get('Header 1', "") + "\n" + c.metadata.get('Header 2', "") + "\n" \
            + c.metadata.get('Header 3', "") + "\n" + c.metadata.get('Header 4', "") + "\n" \
            + c.page_content
        content = content.strip()
        if len(tokenizer.encode(previous_chunk + content)) <= max_chunk_size/2:
            previous_chunk += content + "\n"
        else:
            chunks.extend(text_splitter.split_text(previous_chunk.strip()))
            previous_chunk = content + "\n"
    if previous_chunk:
        chunks.extend(text_splitter.split_text(previous_chunk.strip()))
    # Discard too small chunks
    return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]

# COMMAND ----------

md = df.limit(1).collect()[0]['content_string']

# COMMAND ----------

chunks = split_md(md)

# COMMAND ----------

len(chunks)

# COMMAND ----------

chunks[0]

# COMMAND ----------

chunks[1]

# COMMAND ----------

chunks[2]

# COMMAND ----------

df.printSchema()

# COMMAND ----------

# MAGIC %sql
# MAGIC --Note that we need to enable Change Data Feed on the table to create the index
# MAGIC CREATE TABLE IF NOT EXISTS edgar_form (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   accession_no STRING,
# MAGIC   path STRING,
# MAGIC   form STRING,
# MAGIC   filing_date DATE,
# MAGIC   cik LONG,
# MAGIC   name STRING,
# MAGIC   display_name STRING,
# MAGIC   tickers STRING,
# MAGIC   exchanges STRING,
# MAGIC   industry STRING,
# MAGIC   content STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

@pandas_udf("array<string>")
def parse_and_split(docs: pd.Series) -> pd.Series:
    return docs.apply(split_md)

# COMMAND ----------

(df
    .withColumn('content', explode(parse_and_split('content_string')))
    .drop("modificationTime", "length", "content_string")
    .write.mode('overwrite').saveAsTable("edgar_form"))

# COMMAND ----------

display(spark.table("edgar_form"))

# COMMAND ----------

from databricks.vector_search.client import VectorSearchClient
vsc = VectorSearchClient()

VECTOR_SEARCH_ENDPOINT_NAME="edgar_vs_endpoint"

vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type="STANDARD")

# COMMAND ----------

from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c

#The table we'd like to index
source_table_fullname = f"llm_hackathon.default.edgar_form"
# Where we want to store our index
vs_index_fullname = f"llm_hackathon.default.edgar_form_vs_index"

vsc.create_delta_sync_index(
    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
    index_name=vs_index_fullname,
    source_table_name=source_table_fullname,
    pipeline_type="TRIGGERED",
    primary_key="id",
    embedding_source_column='content', #The column containing our text
    embedding_model_endpoint_name='databricks-bge-large-en' #The embedding endpoint used to create the embeddings
)

# COMMAND ----------

from langchain_community.vectorstores import DatabricksVectorSearch
from langchain_community.embeddings import DatabricksEmbeddings

# COMMAND ----------

embedding_model = DatabricksEmbeddings(endpoint="databricks-bge-large-en")

vs_index = vsc.get_index(
    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
    index_name=vs_index_fullname
)

# Create the retriever
vectorstore = DatabricksVectorSearch(
    vs_index, text_column="content", embedding=embedding_model,
    columns=['name', 'tickers', 'form']
)

naive_retriever = vectorstore.as_retriever(search_kwargs={"k" : 5})

# COMMAND ----------

similar_documents = naive_retriever.get_relevant_documents("How many gpus are we planning to sell?")

# COMMAND ----------

similar_documents
